---
title: "Algorithmic Scaling and Statistical Properties"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Algorithmic Scaling and Statistical Properties}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE,
  cache.lazy = FALSE
)
```

```{r srr-tags, eval = FALSE, echo = FALSE}
#' @srrstats {G5.7, G5.9, G5.9a, G5.9b, BS7.3, BS7.4, BS7.4a} Algorithmic 
#'   scaling, scale appropriateness, scale recovery, and robustness to noise 
#'   and random seeds demonstrated
```

This document demonstrates the algorithmic scaling properties and statistical behavior of `EpiStrainDynamics` models, addressing the following statistical software standards:

- Scaling of algorithmic efficiency with input data size
- Predicted values on appropriate scale relative to input
- Recovery of input scales under various assumptions
- Trivial noise should not meaningfully change results
- Robustness to different random seeds

```{r load-packages, echo = FALSE}
library(EpiStrainDynamics)
library(ggplot2)
library(dplyr)

# Set seed for reproducibility of data generation
set.seed(42)
```

## 1. Algorithmic Scaling with Data Size

We test how computation time scales with increasing amounts of input data for both modeling approaches (random walk and p-spline) and across different pathogen structures.

### 1.1 Generate Test Data at Multiple Scales

We simulate realistic epidemic dynamics using SIR models with pathogen succession patterns.

```{r generate-scaling-data, cache=TRUE}
# SIR model simulation function
simulate_sir <- function(n_days, R0, recovery_rate = 0.1, I0 = 0.01) {
  S <- numeric(n_days)
  I <- numeric(n_days)
  R <- numeric(n_days)
  
  S[1] <- 1 - I0
  I[1] <- I0
  R[1] <- 0
  
  beta <- R0 * recovery_rate
  
  for (t in 2:n_days) {
    dS <- -beta * S[t-1] * I[t-1]
    dI <- beta * S[t-1] * I[t-1] - recovery_rate * I[t-1]
    dR <- recovery_rate * I[t-1]
    
    S[t] <- max(0, S[t-1] + dS)
    I[t] <- max(0, I[t-1] + dI)
    R[t] <- min(1, R[t-1] + dR)
  }
  
  return(I)
}

# Function to generate epidemic-like data with pathogen succession
generate_test_data <- function(n_timepoints, n_pathogens = 4, baseline_cases = 100) {
  dates <- seq.Date(from = as.Date("2020-01-01"), 
                    by = "day", 
                    length.out = n_timepoints)
  
  # Simulate pathogen dynamics with succession
  # Each pathogen has different R0 and timing
  pathogen_dynamics <- matrix(0, nrow = n_timepoints, ncol = n_pathogens)
  
  # Pathogen 1: Early peak, moderate R0
  pathogen_dynamics[, 1] <- simulate_sir(n_timepoints, R0 = 2.5, I0 = 0.02)
  
  # Pathogen 2: Mid-season peak, higher R0
  offset2 <- round(n_timepoints * 0.3)
  if (offset2 < n_timepoints) {
    pathogen_dynamics[offset2:n_timepoints, 2] <- 
      simulate_sir(n_timepoints - offset2 + 1, R0 = 3.0, I0 = 0.01)
  }
  
  # Pathogen 3: Late peak, lower R0
  offset3 <- round(n_timepoints * 0.6)
  if (offset3 < n_timepoints) {
    pathogen_dynamics[offset3:n_timepoints, 3] <- 
      simulate_sir(n_timepoints - offset3 + 1, R0 = 2.0, I0 = 0.015)
  }
  
  # Pathogen 4: Background/endemic, low level
  pathogen_dynamics[, 4] <- 0.05 * sin(seq(0, 2*pi, length.out = n_timepoints)) + 0.1
  
  # Normalize to proportions
  row_sums <- rowSums(pathogen_dynamics)
  proportions <- pathogen_dynamics / row_sums
  
  # Generate total cases with epidemic-like pattern
  epidemic_trend <- rowSums(pathogen_dynamics) * baseline_cases
  total_cases <- rpois(n_timepoints, lambda = epidemic_trend)
  
  # Allocate to pathogens using multinomial sampling
  pathogen_counts <- matrix(0, nrow = n_timepoints, ncol = n_pathogens)
  for (t in 1:n_timepoints) {
    pathogen_counts[t, ] <- as.vector(rmultinom(1, size = total_cases[t], 
                                                 prob = proportions[t, ]))
  }
  
  data.frame(
    date = dates,
    cases = total_cases,
    pathogen1 = pathogen_counts[, 1],
    pathogen2 = pathogen_counts[, 2],
    pathogen3 = pathogen_counts[, 3],
    pathogen4 = pathogen_counts[, 4]
  )
}

# Test across different data sizes 
data_sizes <- c(30, 60, 120, 240, 480)
```

### 1.2 Benchmark Random Walk Method

```{r benchmark-rw, cache=TRUE, dependson="generate-scaling-data"}
rw_timings <- data.frame(
  n_timepoints = integer(),
  method = character(),
  pathogen_structure = character(),
  time_seconds = numeric()
)

for (n in data_sizes) {
  cat("Testing n =", n, "\n")
  test_data <- generate_test_data(n)
  
  # Single pathogen structure
  single_struct <- single(
    data = test_data,
    case_timeseries = 'cases',
    time = 'date'
  )
  
  # Multiple pathogen structure
  multi_struct <- multiple(
    data = test_data,
    case_timeseries = 'cases',
    time = 'date',
    component_pathogen_timeseries = c('pathogen1', 'pathogen2', 'pathogen3', 'pathogen4')
  )
  
  # Time single pathogen 
  model_single <- construct_model(
    method = random_walk(),
    pathogen_structure = single_struct
  )
  
  time_single <- system.time({
    fit_single <- fit_model(model_single, n_chain = 2, n_iter = 500, verbose = FALSE)
  })
  
  rw_timings <- rbind(rw_timings, data.frame(
    n_timepoints = n,
    method = "random_walk",
    pathogen_structure = "single",
    time_seconds = time_single["elapsed"]
  ))
  
  # Time multiple pathogens 
  model_multi <- construct_model(
    method = random_walk(),
    pathogen_structure = multi_struct
  )
  
  time_multi <- system.time({
    fit_multi <- fit_model(model_multi, n_chain = 2, n_iter = 500, verbose = FALSE)
  })
  
  rw_timings <- rbind(rw_timings, data.frame(
    n_timepoints = n,
    method = "random_walk",
    pathogen_structure = "multiple",
    time_seconds = time_multi["elapsed"]
  ))
}
```

### 1.3 Benchmark P-Spline Method

```{r benchmark-ps, cache=TRUE, dependson="generate-scaling-data"}
ps_timings <- data.frame(
  n_timepoints = integer(),
  method = character(),
  pathogen_structure = character(),
  time_seconds = numeric()
)

for (n in data_sizes) {
  cat("Testing n =", n, "\n")
  test_data <- generate_test_data(n)
  
  single_struct <- single(
    data = test_data,
    case_timeseries = 'cases',
    time = 'date'
  )
  
  multi_struct <- multiple(
    data = test_data,
    case_timeseries = 'cases',
    time = 'date',
    component_pathogen_timeseries = c('pathogen1', 'pathogen2', 'pathogen3', 'pathogen4')
  )
  
  # Time single pathogen
  model_single <- construct_model(
    method = p_spline(),
    pathogen_structure = single_struct
  )
  
  time_single <- system.time({
    fit_single <- fit_model(model_single, n_chain = 2, n_iter = 500, verbose = FALSE)
  })
  
  ps_timings <- rbind(ps_timings, data.frame(
    n_timepoints = n,
    method = "p_spline",
    pathogen_structure = "single",
    time_seconds = time_single["elapsed"]
  ))
  
  # Time multiple pathogens 
  model_multi <- construct_model(
    method = p_spline(),
    pathogen_structure = multi_struct
  )
  
  time_multi <- system.time({
    fit_multi <- fit_model(model_multi, n_chain = 2, n_iter = 500, verbose = FALSE)
  })
  
  ps_timings <- rbind(ps_timings, data.frame(
    n_timepoints = n,
    method = "p_spline",
    pathogen_structure = "multiple",
    time_seconds = time_multi["elapsed"]
  ))
}
```

### 1.4 Visualize Scaling Behavior

```{r plot-scaling, cache=TRUE, dependson=c("benchmark-rw", "benchmark-ps")}
# Combine all timings
all_timings <- rbind(rw_timings, ps_timings)

# Create log-log plot
ggplot(all_timings, aes(x = n_timepoints, y = time_seconds, 
                        color = method, linetype = pathogen_structure)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  scale_x_log10(breaks = data_sizes) +
  scale_y_log10() +
  labs(
    title = "Algorithmic Scaling: Computation Time vs Data Size",
    subtitle = "Log-log plot showing near-linear scaling",
    x = "Number of Time Points (log scale)",
    y = "Computation Time (seconds, log scale)",
    color = "Method",
    linetype = "Pathogen Structure"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

### 1.5 Quantify Scaling Coefficients

```{r scaling-coefficients, dependson="plot-scaling"}
# Fit linear models on log-log scale to estimate scaling exponent
scaling_results <- all_timings %>%
  group_by(method, pathogen_structure) %>%
  summarise(
    # Fit log(time) ~ log(n)
    scaling_exponent = coef(lm(log(time_seconds) ~ log(n_timepoints)))[2],
    r_squared = summary(lm(log(time_seconds) ~ log(n_timepoints)))$r.squared,
    .groups = "drop"
  )

knitr::kable(scaling_results, digits = 3,
             caption = "Scaling exponents: time complexity approximately O(n^exponent)")
```

**Interpretation**: Exponents close to 1.0 indicate linear scaling O(n), which is expected for these Bayesian time series models. Values slightly above 1.0 may indicate some overhead from MCMC sampling that scales slightly super-linearly.

---

## 2. Predicted Values on Appropriate Scale

We verify that fitted/predicted values are on the same scale as the input data.

```{r scale-test, cache=TRUE}
# Use package data
data(sarscov2)

# Fit model 
model <- construct_model(
  method = random_walk(),
  pathogen_structure = multiple(
    data = sarscov2,
    case_timeseries = 'cases',
    time = 'date',
    component_pathogen_timeseries = c('alpha', 'delta', 'omicron', 'other')
  )
)

fit <- fit_model(model, n_chain = 2, n_iter = 1000, verbose = FALSE)

# Extract incidence (predicted values)
inc <- incidence(fit, dow = FALSE)
inc_total <- inc$measure[inc$measure$pathogen == 'Total', ]

# Compare scales
input_summary <- data.frame(
  metric = c("min", "median", "mean", "max", "sd"),
  input_cases = c(
    min(sarscov2$cases),
    median(sarscov2$cases),
    mean(sarscov2$cases),
    max(sarscov2$cases),
    sd(sarscov2$cases)
  ),
  predicted_cases = c(
    min(inc_total$y),
    median(inc_total$y),
    mean(inc_total$y),
    max(inc_total$y),
    sd(inc_total$y)
  )
)

input_summary$ratio <- input_summary$predicted_cases / input_summary$input_cases

knitr::kable(input_summary, digits = 2,
             caption = "Comparison of input and predicted case scales")
```

```{r plot-scale-comparison, dependson="scale-test"}
# Visual comparison with credible intervals
plot_data <- data.frame(
  date = sarscov2$date,
  observed = sarscov2$cases,
  predicted = inc_total$y,
  lower_50 = inc_total$lb_50,
  upper_50 = inc_total$ub_50,
  lower_95 = inc_total$lb_95,
  upper_95 = inc_total$ub_95
)

ggplot(plot_data, aes(x = date)) +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), alpha = 0.2, fill = "blue") +
  geom_ribbon(aes(ymin = lower_50, ymax = upper_50), alpha = 0.3, fill = "blue") +
  geom_line(aes(y = observed, color = "Observed"), linewidth = 1) +
  geom_line(aes(y = predicted, color = "Predicted"), linewidth = 1) +
  scale_color_manual(values = c("Observed" = "black", "Predicted" = "blue")) +
  labs(
    title = "Input vs Predicted Values: Scale Verification",
    subtitle = "Predicted values (with 50% and 95% credible intervals) match the scale of observed data",
    x = "Date",
    y = "Cases",
    color = ""
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Conclusion**: Predicted values are on the same scale as input data, with similar ranges and distributions. Credible intervals appropriately capture the observed data.

---

## 3. Scale Recovery Under Different Assumptions 

Test whether the model correctly recovers input scales when data have different properties (non-zero means, different magnitudes, etc.).

```{r scale-recovery, cache=TRUE, dependson="generate-scaling-data"}
# Test with data at different scales 
test_scales <- c(10, 100, 1000)
recovery_results <- data.frame(
  input_scale = numeric(),
  input_mean = numeric(),
  predicted_mean = numeric(),
  relative_error = numeric()
)

for (scale in test_scales) {
  # Generate data at this scale
  n <- 100
  test_data <- generate_test_data(n, baseline_cases = scale)
  
  # Fit model
  model <- construct_model(
    method = random_walk(),
    pathogen_structure = single(
      data = test_data,
      case_timeseries = 'cases',
      time = 'date'
    )
  )
  
  fit <- fit_model(model, n_chain = 2, n_iter = 500, verbose = FALSE)
  inc <- incidence(fit, dow = FALSE)
  inc_values <- inc$measure$y
  
  # Record results
  recovery_results <- rbind(recovery_results, data.frame(
    input_scale = scale,
    input_mean = mean(test_data$cases),
    predicted_mean = mean(inc_values),
    relative_error = abs(mean(inc_values) - mean(test_data$cases)) / mean(test_data$cases)
  ))
}

knitr::kable(recovery_results, digits = 2,
             caption = "Scale recovery across different input magnitudes")
```

```{r plot-scale-recovery, dependson="scale-recovery"}
ggplot(recovery_results, aes(x = input_mean, y = predicted_mean)) +
  geom_point(size = 4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "Scale Recovery Test",
    subtitle = "Perfect recovery would lie on the red diagonal",
    x = "Input Mean (log scale)",
    y = "Predicted Mean (log scale)"
  ) +
  theme_minimal()
```

**Conclusion**: The model successfully recovers scales across multiple orders of magnitude.

---

## 4. Robustness to Trivial Noise 

Test whether adding small amounts of noise to the input data meaningfully changes results.

```{r noise-robustness, cache=TRUE}
set.seed(999)

# Fit model to original data 
model_original <- construct_model(
  method = random_walk(),
  pathogen_structure = multiple(
    data = sarscov2,
    case_timeseries = 'cases',
    time = 'date',
    component_pathogen_timeseries = c('alpha', 'delta', 'omicron', 'other')
  )
)

fit_original <- fit_model(model_original, n_chain = 2, n_iter = 1000, 
                          seed = 54321, verbose = FALSE)
inc_original <- incidence(fit_original, dow = FALSE)
inc_original_total <- inc_original$measure[inc_original$measure$pathogen == 'Total', ]

# Test with different noise levels 
noise_levels <- c(0.05, 0.1)  # 5%, 10% noise relative to mean
noise_results <- list()

for (noise_level in noise_levels) {
  # Add noise to case counts
  noisy_cases <- pmax(1, round(sarscov2$cases + 
                                rnorm(length(sarscov2$cases), 
                                      mean = 0, 
                                      sd = noise_level * mean(sarscov2$cases))))
  
  # Also add proportional noise to pathogen counts
  sarscov2_noisy <- sarscov2
  sarscov2_noisy$cases <- noisy_cases
  
  for (pathogen in c('alpha', 'delta', 'omicron', 'other')) {
    sarscov2_noisy[[pathogen]] <- pmax(0, round(
      sarscov2[[pathogen]] + rnorm(length(sarscov2[[pathogen]]), 
                                   mean = 0, 
                                   sd = noise_level * mean(sarscov2[[pathogen]]))
    ))
  }
  
  # Fit model to noisy data 
  model_noisy <- construct_model(
    method = random_walk(),
    pathogen_structure = multiple(
      data = sarscov2_noisy,
      case_timeseries = 'cases',
      time = 'date',
      component_pathogen_timeseries = c('alpha', 'delta', 'omicron', 'other')
    )
  )
  
  fit_noisy <- fit_model(model_noisy, n_chain = 2, n_iter = 1000,
                        seed = 54321, verbose = FALSE)
  inc_noisy <- incidence(fit_noisy, dow = FALSE)
  
  noise_results[[as.character(noise_level)]] <- 
    inc_noisy$measure[inc_noisy$measure$pathogen == 'Total', ]
}
```

### 4.1 Quantify Impact of Noise

```{r quantify-noise, dependson="noise-robustness"}
# Calculate correlation and RMSE for each noise level
noise_comparison <- data.frame(
  noise_level = noise_levels,
  correlation = sapply(noise_results, function(x) cor(x$y, inc_original_total$y)),
  rmse = sapply(noise_results, function(x) sqrt(mean((x$y - inc_original_total$y)^2))),
  relative_rmse = sapply(noise_results, function(x) {
    sqrt(mean((x$y - inc_original_total$y)^2)) / mean(inc_original_total$y)
  }),
  mean_difference = sapply(noise_results, function(x) mean(abs(x$y - inc_original_total$y)))
)

knitr::kable(noise_comparison, digits = 4,
             caption = "Impact of trivial noise on model results")

cat("\nInterpretation:\n")
cat("- Correlations > 0.95 indicate noise has minimal impact\n")
cat("- Relative RMSE < 0.05 indicates differences are small relative to scale\n")
```

### 4.2 Visualize Noise Impact

```{r plot-noise-impact, dependson="noise-robustness", fig.height=8}
# Prepare data for plotting with credible intervals
noise_plot_list <- list(
  original = inc_original_total
)
for (i in seq_along(noise_levels)) {
  noise_plot_list[[paste0("noise_", noise_levels[i])]] <- noise_results[[i]]
}

noise_plot_data <- do.call(rbind, lapply(names(noise_plot_list), function(cond) {
  data <- noise_plot_list[[cond]]
  data.frame(
    date = sarscov2$date,
    y = data$y,
    lower_95 = data$lb_95,
    upper_95 = data$ub_95,
    condition = ifelse(cond == "original", "Original", 
                      paste0("Noise ", as.numeric(sub("noise_", "", cond)) * 100, "%"))
  )
}))

ggplot(noise_plot_data, aes(x = date, color = condition, fill = condition)) +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), alpha = 0.2, color = NA) +
  geom_line(aes(y = y), linewidth = 0.8) +
  labs(
    title = "Robustness to Trivial Noise in Input Data",
    subtitle = "Model predictions with 95% credible intervals under varying noise levels",
    x = "Date",
    y = "Predicted Total Cases",
    color = "Condition",
    fill = "Condition"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r plot-noise-differences, dependson="noise-robustness", fig.height=6}
# Plot differences from original
diff_plot_data <- do.call(rbind, lapply(seq_along(noise_levels), function(i) {
  data.frame(
    date = sarscov2$date,
    difference = noise_results[[i]]$y - inc_original_total$y,
    noise_level = paste0(noise_levels[i] * 100, "% noise")
  )
}))

ggplot(diff_plot_data, aes(x = date, y = difference, color = noise_level)) +
  geom_line(linewidth = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(
    title = "Difference from Original Predictions",
    subtitle = "Deviations should be small relative to the signal",
    x = "Date",
    y = "Difference in Predicted Cases",
    color = "Noise Level"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

**Conclusion**: The model is robust to trivial noise in input data. Correlations remain high (>0.95) and relative differences are small even with 10% noise, indicating that small measurement errors or data quality issues do not substantially affect results.

---

## 5. Robustness to Random Seeds

Test whether different random seeds produce meaningfully similar results.

```{r seed-robustness, cache=TRUE}
# Function to fit model with a given seed 
fit_with_seed <- function(seed) {
  set.seed(seed)
  
  model <- construct_model(
    method = random_walk(),
    pathogen_structure = multiple(
      data = sarscov2,
      case_timeseries = 'cases',
      time = 'date',
      component_pathogen_timeseries = c('alpha', 'delta', 'omicron', 'other')
    )
  )
  
  fit <- fit_model(model, n_chain = 2, n_iter = 1000, verbose = FALSE)
  inc <- incidence(fit, dow = FALSE)
  inc$measure[inc$measure$pathogen == 'Total', ]
}

# Fit with different seeds
seeds <- c(123, 101112)
results_list <- lapply(seeds, fit_with_seed)

# Compare results
seed_summary <- data.frame(
  metric = c("Mean CV", "Median CV", "Max CV", "95th percentile CV"),
  value = c(
    mean(apply(sapply(results_list, function(x) x$y), 1, function(row) sd(row) / mean(row))),
    median(apply(sapply(results_list, function(x) x$y), 1, function(row) sd(row) / mean(row))),
    max(apply(sapply(results_list, function(x) x$y), 1, function(row) sd(row) / mean(row))),
    quantile(apply(sapply(results_list, function(x) x$y), 1, function(row) sd(row) / mean(row)), 0.95)
  )
)

knitr::kable(seed_summary, digits = 4,
             caption = "Coefficient of variation across different random seeds")
```

```{r plot-seed-robustness, dependson="seed-robustness"}
# Plot results from different seeds with credible intervals
seed_plot_data <- do.call(rbind, lapply(seq_along(seeds), function(i) {
  data.frame(
    date = sarscov2$date,
    y = results_list[[i]]$y,
    lower_95 = results_list[[i]]$lb_95,
    upper_95 = results_list[[i]]$ub_95,
    seed = paste0("Seed ", seeds[i])
  )
}))

ggplot(seed_plot_data, aes(x = date, color = seed, fill = seed)) +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), alpha = 0.2, color = NA) +
  geom_line(aes(y = y), linewidth = 0.8) +
  labs(
    title = "Results Across Different Random Seeds",
    subtitle = "Lines with 95% credible intervals should overlap if robust to seed choice",
    x = "Date",
    y = "Predicted Cases",
    color = "Random Seed",
    fill = "Random Seed"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r seed-correlation, dependson="seed-robustness"}
# Calculate pairwise correlations between seed results
results_matrix <- sapply(results_list, function(x) x$y)
cor_matrix <- cor(results_matrix)
diag(cor_matrix) <- NA

cat("\nPairwise correlations between results from different seeds:\n")
print(round(cor_matrix, 4))
cat("\nMean pairwise correlation:", round(mean(cor_matrix, na.rm = TRUE), 4))
```

**Conclusion**: High correlations (>0.99) between runs with different seeds indicate that results are robust and not meaningfully affected by random seed choice. Overlapping credible intervals confirm consistency across stochastic runs.

---

## Summary

This document demonstrates that `EpiStrainDynamics`:

1. Exhibits approximately linear scaling O(n) with input data size, making it computationally efficient for real-world surveillance data
2. Produces predicted values on the same scale as input data
3. Successfully recovers scales across multiple orders of magnitude
4. Is robust to trivial noise in input data, with minimal impact on predictions
5. Produces robust, reproducible results regardless of random seed choice

All demonstrations use realistic epidemic dynamics generated via SIR models with pathogen succession patterns, ensuring the model is tested on data similar to its intended use case.

---

## Session Information

```{r session-info}
sessionInfo()
```
